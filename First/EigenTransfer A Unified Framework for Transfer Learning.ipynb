{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Generating data for Algorithm Project</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "from nltk.stem.porter import *\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import string\n",
    "\n",
    "files1 = glob.glob('sraa/realaviation/*')\n",
    "files2 = glob.glob('sraa/simaviation/*')\n",
    "files3 = glob.glob('sraa/realauto/*')\n",
    "files4 = glob.glob('sraa/simauto/*')\n",
    "\n",
    "\n",
    "utf8 = ['\\x83', '\\x93', '\\x97', '\\xa3', '\\xa7', '\\xab', '\\xaf', '\\xb3', '\\xb7', '\\xbb', '\\xbf', '\\xc3', \n",
    "        '\\xc7', '\\xcb', '\\xcf', '\\xd3', '\\xd7', '\\xdb', '\\xdf', '\\xe3', '\\xe7', '\\xeb', '\\xef', '\\xf3', \n",
    "        '\\xf7', '\\xfb', '\\x84', '\\x88', '\\x90', '\\x94', '\\x9c', '\\xa0', '\\xa4', '\\xa8', '\\xac', '\\xb0', \n",
    "        '\\xb4', '\\xb8', '\\xbc', '\\xc0', '\\xc4', '\\xc8', '\\xcc', '\\xd0', '\\xd4', '\\xd8', '\\xdc', '\\xe0', \n",
    "        '\\xe4', '\\xe8', '\\xec', '\\xf0', '\\xf4', '\\xf8', '\\xfc', '\\x85', '\\x91', '\\x95', '\\x99', '\\xa1', \n",
    "        '\\xa5', '\\xa9', '\\xad', '\\xb1', '\\xb5', '\\xb9', '\\xbd', '\\xc1', '\\xc5', '\\xc9', '\\xcd', '\\xd1', \n",
    "        '\\xd5', '\\xd9', '\\xdd', '\\xe1', '\\xe5', '\\xe9', '\\xed', '\\xf1', '\\xf5', '\\xf9', '\\xfd', '\\x92', \n",
    "        '\\x96', '\\xa2', '\\xa6', '\\xaa', '\\xae', '\\xb2', '\\xb6', '\\xba', '\\xbe', '\\xc2', '\\xc6', '\\xca', '\\xce', \n",
    "        '\\xd2', '\\xd6', '\\xda', '\\xde', '\\xe2', '\\xe6', '\\xea', '\\xee', '\\xf2', '\\xf6', '\\xfa', '\\xfe']\n",
    "\n",
    "directory = glob.glob('sraa/*')\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sraa\\realauto\n",
      "sraa\\realaviation\n",
      "sraa\\simauto\n",
      "sraa\\simaviation\n"
     ]
    }
   ],
   "source": [
    "documents = []\n",
    "\n",
    "for folder in directory:\n",
    "    print folder\n",
    "    tmp = []\n",
    "    files = glob.glob(folder+\"/*\")\n",
    "    nums = 0\n",
    "    for fileName in files:\n",
    "        f = open(fileName,'r')\n",
    "        data = f.read()\n",
    "        data = ''.join(i if ord(i)<128 else ' ' for i in data )\n",
    "        data = data.lower()\n",
    "        \n",
    "        #toker = RegexpTokenizer(r'((?<=[^\\w\\s])\\w(?=[^\\w\\s])|(\\W))+', gaps=True)\n",
    "        #dataA = toker.tokenize(data)\n",
    "        \n",
    "        dataA = word_tokenize(data)\n",
    "        dataA_without_punc = [i for i in dataA if i not in string.punctuation]\n",
    "        \n",
    "        single = ' '.join([stemmer.stem(w) for w in dataA_without_punc])\n",
    "        tmp.append(single)\n",
    "    documents.append(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from collections import defaultdict\n",
    "from gensim import corpora,models, similarities\n",
    "\n",
    "def preprocess(documents):\n",
    "    # documents = ['babak is khar','hasan is good']\n",
    "    stopList = stopwords.words('english')\n",
    "    print len(stopList)\n",
    "    texts = [[word for word in document.lower().split() if word not in stopList]\n",
    "         for document in documents]\n",
    "    frequency = defaultdict(int)\n",
    "    for text in texts:\n",
    "        for token in text:\n",
    "            frequency[token] += 1\n",
    "\n",
    "    return [[token for token in text if frequency[token] > 1]\n",
    "        for text in texts]\n",
    "    \n",
    "    \n",
    "    \n",
    "def makeCorpus(texts):\n",
    "    # texts = [['babak', 'is' , 'khar'],['hasan', 'is','good']]\n",
    "    # it means we do some preprocess like: stopword, common word, ...\n",
    "    dictionary = corpora.Dictionary(texts)\n",
    "#     print dictionary.token2id\n",
    "    return [dictionary.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "127\n"
     ]
    }
   ],
   "source": [
    "alldocs = []\n",
    "for docs in documents:\n",
    "    for doc in docs:\n",
    "        alldocs.append(doc)\n",
    "\n",
    "        \n",
    "texts = preprocess(alldocs)\n",
    "corpus = makeCorpus(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63\n"
     ]
    }
   ],
   "source": [
    "print len(corpus[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (2, 4), (9, 1), (10, 1), (11, 3), (12, 1), (13, 1), (14, 1), (16, 4), (18, 1), (23, 1), (28, 1), (38, 1), (44, 1), (45, 1), (46, 2), (47, 1), (50, 1), (51, 1), (53, 4), (54, 4), (58, 2), (60, 1), (81, 2), (100, 1), (114, 1), (121, 2), (139, 9), (144, 1), (149, 1), (152, 2), (155, 1), (163, 1), (164, 1), (165, 1), (166, 5), (167, 1), (183, 1), (185, 1), (187, 2), (191, 4), (193, 2), (205, 1), (224, 1), (236, 1), (259, 4), (270, 1), (289, 1), (314, 1), (315, 1), (318, 1), (319, 1), (320, 1), (323, 2), (324, 1), (325, 1), (329, 1), (332, 1), (337, 1), (340, 1), (345, 1), (347, 1), (348, 1), (349, 1), (350, 1), (351, 1), (352, 1), (353, 1), (354, 1), (356, 2), (362, 3), (363, 1), (365, 1), (367, 1), (370, 1), (371, 1), (374, 1), (375, 4), (376, 1), (378, 1), (379, 1), (381, 1), (385, 1), (386, 1), (387, 2), (388, 2), (390, 1), (394, 1), (395, 1), (401, 1), (404, 1), (416, 1), (515, 1), (517, 1), (528, 1), (539, 2), (553, 2), (555, 1), (556, 2), (558, 1), (559, 1), (560, 1), (561, 1), (576, 1), (578, 1), (579, 2), (580, 1), (582, 1), (584, 2), (586, 1), (587, 1), (588, 2), (592, 1), (594, 1), (598, 4), (603, 1), (606, 1), (609, 1), (611, 1), (612, 1), (613, 2), (614, 1), (615, 1), (616, 1), (617, 1), (618, 1), (619, 1), (620, 1), (621, 1), (622, 2), (623, 2), (624, 2), (625, 1), (626, 1), (627, 1), (628, 1), (629, 1), (630, 1), (631, 1), (632, 1), (633, 1), (634, 1), (635, 1), (636, 1), (637, 1), (638, 1), (639, 1), (640, 1), (641, 1), (642, 1), (643, 1), (644, 1), (645, 1), (646, 1), (647, 2), (648, 1), (649, 1), (650, 1), (651, 1), (652, 1), (653, 1), (654, 1), (655, 1)]\n"
     ]
    }
   ],
   "source": [
    "print corpus[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-b3066378b511>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mtransformer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTfidfTransformer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[0mtrainVectorizerArray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[0mtestVectorizerArray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_set\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[1;34m'Fit Vectorizer to train set'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainVectorizerArray\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Anaconda\\lib\\site-packages\\sklearn\\feature_extraction\\text.pyc\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m    815\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    816\u001b[0m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[1;32m--> 817\u001b[1;33m                                           self.fixed_vocabulary_)\n\u001b[0m\u001b[0;32m    818\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    819\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Anaconda\\lib\\site-packages\\sklearn\\feature_extraction\\text.pyc\u001b[0m in \u001b[0;36m_count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m    746\u001b[0m         \u001b[0mindptr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    747\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 748\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[1;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    749\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    750\u001b[0m                     \u001b[0mj_indices\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Anaconda\\lib\\site-packages\\sklearn\\feature_extraction\\text.pyc\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(doc)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    233\u001b[0m             return lambda doc: self._word_ngrams(\n\u001b[1;32m--> 234\u001b[1;33m                 tokenize(preprocess(self.decode(doc))), stop_words)\n\u001b[0m\u001b[0;32m    235\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    236\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Anaconda\\lib\\site-packages\\sklearn\\feature_extraction\\text.pyc\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    199\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlowercase\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 200\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstrip_accents\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    201\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    202\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mstrip_accents\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# train_set = (documents) #Documents\n",
    "train_set = (alldocs) #Documents\n",
    "\n",
    "test_set = ([\"Content-Type text 7bit news.\"]) #Query\n",
    "stopWords = stopwords.words('english')\n",
    "\n",
    "vectorizer = CountVectorizer(stop_words = stopWords)\n",
    "transformer = TfidfTransformer()\n",
    "\n",
    "trainVectorizerArray = vectorizer.fit_transform(train_set).toarray()\n",
    "testVectorizerArray = vectorizer.transform(test_set).toarray()\n",
    "print 'Fit Vectorizer to train set', trainVectorizerArray\n",
    "print 'Transform Vectorizer to test set', testVectorizerArray\n",
    "\n",
    "transformer.fit(trainVectorizerArray)\n",
    "print transformer.transform(trainVectorizerArray).toarray()\n",
    "\n",
    "#transformer.fit(testVectorizerArray)\n",
    "\n",
    "#tfidf = transformer.transform(testVectorizerArray)\n",
    "#print 'dens\n",
    "\n",
    "#e'\n",
    "#print tfidf.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopWords = stopwords.words(['english'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path: news.jprc.com!newsfeed.sgi.net!uwm.edu!math.ohio-state.edu!howland.erols.net!wn3feed!worldnet.att.net!135.173.83.225!attworldnet!newsadm\n",
      "From: news@4x4review.com (4X4REVIEW)\n",
      "Newsgroups: rec.autos.misc\n",
      "Subject: !!!!!!NEW WEBSITE 4x4/SUV product and vehicle reviews and MUCH MORE!!!!!!!!!!!!\n",
      "Date: 24 Sep 1998 04:03:13 GMT\n",
      "Organization: AT&T WorldNet Services\n",
      "Lines: 9\n",
      "Message-ID: <3609c46d.95136777@netnews.worldnet.att.net>\n",
      "NNTP-Posting-Host: 12.64.40.208\n",
      "X-Newsreader: Forte Free Agent 1.11/32.235\n",
      "Xref: news.jprc.com rec.autos.misc:22958\n",
      "\n",
      "http://www.4x4review.com/ng\n",
      "There's a new web site on the net that does 4x4 and SUV product and\n",
      "vehicle reviews.  Next month, be the first to see the review and\n",
      "technical data that goes with Detroit's new GEARLESS LOCKER!\n",
      "In addition, they are doing bi-weekly newsletters, selling books and\n",
      "videos, listing 4x4 clubs all around the world, they have discussion\n",
      "forums and much much more!  Take a peek and see what's new at:\n",
      "http://www.4x4review.com/ng\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print documents[24]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "doc = '''\n",
    "Path: news.jprc.com!newsfeed.sgi.net!uwm.edu!math.ohio-state.edu!\n",
    "howland.erols.net!wn3feed!worldnet.att.net!135.173.83.225!attworldnet!\n",
    "newsadm\\nFrom: news@4x4review.com (4X4REVIEW)\\nNewsgroups: \n",
    "rec.autos.misc\\nSubject: !!!!!!NEW WEBSITE 4x4/SUV product and vehicle reviews and MUCH MORE!!!!!!!!!!!!\\n\n",
    "Date: 24 Sep 1998 04:03:13 GMT\\nOrganization: AT&T WorldNet Services\\nLines: 9\\nMessage-ID:\n",
    "\\nNNTP-Posting-Host: 12.64.40.208\\nX-Newsreader: Forte Free Agent 1.11/32.235\\nXref: \n",
    "news.jprc.com rec.autos.misc:22958\\n\\nhttp://www.4x4review.com/ng\\nThere's a new web site on the net that does 4x4 and SUV \n",
    "product and\\nvehicle reviews.  Next month, be the first to see the review and\\ntechnical data that goes with Detroit\\x92s \n",
    "new GEARLESS LOCKER!\\nIn addition, they are doing bi-weekly newsletters, selling books and\\nvideos, listing 4x4 clubs all around the world, they have discussion\\nforums and much much more!  Take a peek and see what's new at:\\nhttp://www.4x4review.com/ng\\n\\n\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Path: news.jprc.com!newsfeed.sgi.net!uwm.edu!math.ohio-state.edu!\n",
      "howland.erols.net!wn3feed!worldnet.att.net!135.173.83.225!attworldnet!\n",
      "newsadm\n",
      "From: news@4x4review.com (4X4REVIEW)\n",
      "Newsgroups: \n",
      "rec.autos.misc\n",
      "Subject: !!!!!!NEW WEBSITE 4x4/SUV product and vehicle reviews and MUCH MORE!!!!!!!!!!!!\n",
      "\n",
      "Date: 24 Sep 1998 04:03:13 GMT\n",
      "Organization: AT&T WorldNet Services\n",
      "Lines: 9\n",
      "Message-ID:\n",
      "\n",
      "NNTP-Posting-Host: 12.64.40.208\n",
      "X-Newsreader: Forte Free Agent 1.11/32.235\n",
      "Xref: \n",
      "news.jprc.com rec.autos.misc:22958\n",
      "\n",
      "http://www.4x4review.com/ng\n",
      "There's a new web site on the net that does 4x4 and SUV \n",
      "product and\n",
      "vehicle reviews.  Next month, be the first to see the review and\n",
      "technical data that goes with Detroit\\x92s \n",
      "new GEARLESS LOCKER!\n",
      "In addition, they are doing bi-weekly newsletters, selling books and\n",
      "videos, listing 4x4 clubs all around the world, they have discussion\n",
      "forums and much much more!  Take a peek and see what's new at:\n",
      "http://www.4x4review.com/ng\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "name = 'sraa/realauto/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73218\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora,models,similarities\n",
    "newDoc = []\n",
    "for d in documents:\n",
    "    for i in d:\n",
    "        newDoc.append(i)\n",
    "print len(newDoc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = models.Word2Vec(newDoc,size=100,window=5,min_count=5,workers=4)\n",
    "model.save(\"word.txt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
